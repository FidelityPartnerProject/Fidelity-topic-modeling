---
title: "Final version - fund"
author: "Hao He, Huifei Xu"
date: "2022-12-04"
output: html_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(tidytext)
library(ggplot2)
library(topicmodels)
library(quanteda)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(readr)
library(textstem)
```

# Data preprocessing
First, import data and break down into words. Tokenizing at word level. We’re treating every review as a separate “document”. And then we use lemmatization.

```{r}
fund <- readxl::read_xlsx("Fund_Strategy_Data.xlsx")

# clean data 
clean<- function (x , pattern){gsub(pattern, " ", x)}
# remove html tags and new lines symbol
fund$FUND_STRATEGY <- clean(fund$FUND_STRATEGY,"<br>")
fund$FUND_STRATEGY <- clean(fund$FUND_STRATEGY,"\r\r\n")

# filter unique values based on ticker and strategy: 12314
# fund %>% distinct(TCKR_C, FUND_STRATEGY,.keep_all = T)
# this dataframe contains funds with no ticker (NA)
# funds with no ticker contains different strategies, so keep them, use it as the clean fund dataset after remove duplicated funds 


cleanfund <- fund %>% mutate(id = row_number()) %>% select(id,FUND_STRATEGY) 
# %>% distinct(TCKR_C, FUND_STRATEGY,.keep_all = T) %>% select(id,everything())
# cleanfund <- cleanfund %>% select(id,FUND_STRATEGY)

```

# Tokenization and lemmatization
Create a function get_lemma to integrate these steps:
Use clean data to do toeknization by single word. Unnest word and then calculate the count of word frequency after remove the normal stop-words. Next, use lemmatization to convert a word to its original form.

```{r}
# cleandf is a dataframe that contains clean data after data preprocessing
get_lemma <- function(cleandf){
  tidy_words <- cleandf %>% unnest_tokens(word, !!sym(colnames(cleandf)[2])) %>% anti_join(stop_words)
  lemmatization <- tidy_words%>% mutate(lemma = word%>% lemmatize_words())
  return(lemmatization)
}
# get_lemma(cleanfund)
```

# Tf-idf
Create a function get_tf-idf that saves the results of per-word-per-document tf, idf, and tf-idf in a dataframe.
```{r}

get_tf_idf <- function(cleandf){
  tf_idf_table<- cleandf %>%
    get_lemma %>%
    count(id, lemma, sort = TRUE) %>%
    bind_tf_idf(lemma, id, n) %>% arrange(tf_idf)
  return(tf_idf_table)
}

# get_tf_idf(cleanfund)

```

# Automation of stop-words

Stop-words generation process:

1.  Create the td-idf table after lemmatization, sort the tf-idf table in an ascending order

2.  Rank the per-word-per document tf-idf using min_rank() function and pull out words with very low tf-idf score:

-   min_rank() is used to assure that ties are assigned the minimum ranking possible, so the input for the rank is restricted to the smallest possible number. This limits the number of words being removed.

3. Get unique values for the extracted words and save them into a dataframe. 


```{r, message=FALSE}
# use rank table as basis for 80/20, 50/50 split

# create a function to get rank table, max of ranks: 193
# input1: [cleandf] is the data frame contain two main cols "id" and the text content after fundamental text clean
# output: `n` is the number of words in each rank, `cumsum` show the cumulative number of words for each rank

get_ranktable <- function(cleandf){
  ranktable <- cleandf %>% get_lemma()%>% get_tf_idf %>% group_by(!!sym(colnames(cleandf)[1])) %>% 
  mutate(rank = min_rank(tf_idf)) %>% group_by(lemma) %>%
      arrange(rank) %>%
    filter(row_number() == 1)%>% select(id,lemma,rank) %>% ungroup %>% count(rank) %>% mutate(total = cumsum(n)) %>% mutate(cum_percent = round(100*cumsum(n)/sum(n),2))
  return(ranktable)} 

fund_ranktable <- get_ranktable(cleanfund)

# summary(fund_ranktable$rank) 
# Output Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#   1.0     9.0    18.0    24.3    30.0   193.0 



# Now use find_stopwords function:

# input1: [cleandf] is the data frame contain two main cols "id" and the text content after fundamental text clean
# input2: [maxrank] is one integer value. It refers the the maximum rank of tf-idf score for stop words expected to show in output
# out put: a data frame with two vectors: Vector1 is the stopwords; vector2 is the factors of rank from 1 to the [maxrank]
find_stopwords <- function(cleandf, maxrank=0, percent=1){ 
  
  
  # tf_idf_table is the computed tf-idf table using get_tf-idf function that has column names in the order of "id", "lemma","n", "tf", "idf" and "tf_idf" 
  # id is the first column name of the tf-idf table, it refers to the column that lists the unique id of each document
  # x is the ranking of elements order by tf-idf in an ascending order.
  # So x = 1 would extract 1-word tokens with the lowest tf-idf score; x = 2 would get tokens with the second lowest tf-idf score.
  # min_rank used to assure that ties are assigned the minimum ranking possible, so the input for the rank is restricted to the smallest possible number.
  # min_score is a data frame that contains the elements met the ranking demand  
  
  tidy_words <- cleandf %>%
    unnest_tokens(word, !!sym(colnames(cleandf)[2])) %>%
    anti_join(stop_words)
  
  lemmatization <- tidy_words%>% 
    mutate(lemma = word %>% 
             lemmatize_words()
           )
  
  tf_idf_table<- lemmatization %>% 
    count(id, lemma, sort = TRUE) %>% 
    bind_tf_idf(lemma, id, n) %>% 
    arrange(tf_idf)
  
  id = colnames(tf_idf_table)[1] 
  id <- enquo(id)
  
  
  if(maxrank!=0 & percent!=1){
    stop("Please assign rank or percent not both!")
  }
  else if(percent<0 | percent>1){
    stop("Please choose a fraction between 0 and 1!")
  } 
  
  else{
    by_rank <-  tf_idf_table %>% 
      group_by_at(vars(!!id)) %>% 
      mutate(rank = min_rank(tf_idf)) %>%
      group_by(lemma) %>%
      arrange(rank) %>%
    filter(row_number() == 1)

    if(maxrank==0){
      maxrank <- max(by_rank$rank)
    }
    min_score <- by_rank %>% filter(rank <= as.numeric(maxrank))
    
    my_stopwords <- min_score %>% 
    ungroup %>%
    select(lemma,rank)
    
    dup_logic <- duplicated(my_stopwords$lemma)
    ans <- my_stopwords[!dup_logic, ]
  
    
    word_count <- dim(ans)[1]
    target_word_count <- round(percent*word_count)
    ans <-  ans%>% top_n(-target_word_count)

    ans$rank = factor(ans$rank)
    
  }
  return(ans) 

}

# check fund_ranktable

# 80/20 rule
find_stopwords(cleanfund,maxrank=49)


# 20/80 rule
find_stopwords(cleanfund,maxrank=15)

# 50/50 rule
find_stopwords(cleanfund,maxrank= 27)
```


