---
title: "Fund strategy"
author: "Hao He"
date: "2022-11-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(tidytext)
library(ggplot2)
library(topicmodels)
library(quanteda)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(readr)
library(textstem)
```

# Data preprocessing
First, import data and break down into words. Tokenizing at word level. We’re treating every review as a separate “document”. And then we use lemmatization.
```{r, warning=FALSE, message=FALSE}
fund <- readxl::read_xlsx("Fund_Strategy_Data.xlsx")

# clean data 
clean<- function (x , pattern){gsub(pattern, " ", x)}
# remove html tags and new lines symbol
fund$FUND_STRATEGY <- clean(fund$FUND_STRATEGY,"<br>")
fund$FUND_STRATEGY <- clean(fund$FUND_STRATEGY,"\r\r\n")

# # unique rows of `TCKR_C`and `Fund_Strategy` columns
# 
# # fund strategy: 6319 rows
# fund %>% distinct(FUND_STRATEGY)
# 
# # ticker : 9513 rows
# a <- fund%>% distinct(TCKR_C,.keep_all = T)
# 
# 
# # unique values based on ticker and strategy: 12314
# b <- fund %>% distinct(TCKR_C, FUND_STRATEGY,.keep_all = T)
# 
# ## check: which unique fund subset should we use?  
# 
# ## funds with no ticker (NA) 
# dif <- anti_join(b,a)
# 
# # funds with no ticker contains different strategies, so keep them, use b as the clean fund dataset after remove duplicated funds 
```
# tokenization and lemmatization

Use clean data to do toeknization by single word. Unnest word and then calculate the count of word frequency after remove the normal stop-words. Next, use lemmatization to convert a word to its original form.
```{r}
# because there are funds with no ticker, we create a fund id for each fund
cleanfund <- fund %>% mutate(id = row_number()) %>% distinct(TCKR_C, FUND_STRATEGY,.keep_all = T) %>% select(id,everything())
cleanfund <- cleanfund %>% select(id,FUND_STRATEGY)

# cleandf is a dataframe that contains clean data after data preprocessing
lemmatize <- function(cleandf){
  outputcol <- colnames(cleandf)[2]
  tidy_words <- cleandf %>% unnest_tokens(word, sym(outputcol)) %>% anti_join(stop_words)
  lemmatization <- tidy_words%>% mutate(lemma = word%>% lemmatize_words())
  return(lemmatization)
}
f_lemmatization <- lemmatize(cleanfund)
# strategy_words <- cleanfund %>% unnest_tokens(word, FUND_STRATEGY) %>% select(id, word)
# tidy_strategy_words <- strategy_words %>% anti_join(stop_words)

#  word frequency in each document
## most common words 
# tidy_strategy_words %>% count(id,word, sort = TRUE) 
# 
# 
# tidy_strategy_words %>% count(id, word, sort = TRUE) %>% filter(n > 15) %>%
#   mutate(word = reorder(word, n)) %>%
#   ggplot(aes(n, word)) +
#   geom_col() +
#   labs(y = NULL, x = "count of word frequency")

# ## word frequency in the corpus
# ## most common words 
# tidy_strategy_words %>% count(word, sort = TRUE) 
# 
# 
# tidy_strategy_words %>% count(word, sort = TRUE) %>% filter(n > 8000) %>%
#   mutate(word = reorder(word, n)) %>%
#   ggplot(aes(n, word)) +
#   geom_col() +
#   labs(y = NULL, x = "count of word frequency")


## lemmatization and its visualization

# fund_lemmatization <- tidy_strategy_words %>% mutate(lemma = word%>% lemmatize_words())
# 
# fund_lemmatization %>% 
# count(lemma, sort = TRUE) %>% filter(n > 8000) %>%
#   mutate(lemma = reorder(lemma, n)) %>%
#   ggplot(aes(n, lemma, fill = "red")) +
#   geom_col(show.legend = FALSE) +
#   labs(y = NULL,x = "count of word frequency after lemmatization" )

# It can be easily seen that the word frequency of fund increases from approximately 25000 to 35000.
```





# Tf-idf
```{r ticker tf-idf}
# tf
ticker_words<- fund_lemmatization %>% count(fund_id, lemma, sort = TRUE) 

# `total` column means total counts of most common words in each document   
# total_words <- lemmatization %>% count(fund_id, lemma, sort = TRUE) %>% 
#   group_by(fund_id) %>% summarize(total = sum(n))
# 
# ticker_words <- left_join(ticker_words,total_words)
# ticker_words


# calculating tf-idf
fund_tf_idf <- ticker_words %>% bind_tf_idf(lemma, fund_id, n) %>% arrange(tf_idf) 



# tf-idf visualization
library(forcats)

fund_tf_idf %>%
  group_by(fund_id) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(lemma, tf_idf), fill = fund_id)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~fund_id, ncol = 4, scales = "free") +
  labs(x = "tf-idf", y = NULL)

```



# automation of stop_words
for both datasets, without look into the content and the domain specific words
custom_stop_words: cutoff should be carefully choose, depends on how much data we have

*Apply different rules to get stop-words list*

## Rule 1. Based on tf-idf
```{r}
#  rank the tf-idf and remove common words with very low tf-idf score 
min_score <- fund_tf_idf %>% group_by(fund_id) %>% mutate(rank = min_rank(tf_idf)) %>% filter(rank < 1 | rank == 1)
my_stopwords1 <- min_score %>% ungroup %>% distinct(lemma)

# checkg generated stop-words list
my_stopwords1 

# 13 stop-words when rank = 1
# 27 stop-words when rank = 2
```

## Rule 2. Based on idf
```{r}
min_score <- fund_tf_idf %>% group_by(fund_id) %>% mutate(rank = min_rank(idf)) %>% filter(rank < 1 | rank == 1)
my_stopwords2 <- min_score %>% ungroup %>% distinct(lemma)

# check generated stop-words list
my_stopwords2 

# 10 stop-words when rank = 1
# 22 stop-words when rank = 2


```

tf-idf seems to have better results.

Now write a function for faster execution using tf-idf rule
```{r}
# create a general function for automation of stop-words
get_stopwords <- function(tf_idf_table, x){ 
  # tf_idf_table is the tf-idf table has column names in the order of "id", "lemma","n","total", "tf", "idf" and "tf_idf" 
  # id is the first column name of the tf-idf table, it refers to the column that lists the unique id of each document
  # x is the ranking of elements order by tf-idf in an ascending order.
  # So x = 1 would draw out elements with the lowest tf-idf score; x = 2 would get elements with the second lowest tf-idf score.
  # min_score is a data frame that contains the elements met the ranking demand  
id = colnames(tf_idf_table)[1] 
id <- enquo(id)
min_score <- tf_idf_table %>% group_by_at(vars(!!id)) %>% mutate(rank = min_rank(tf_idf)) %>% filter(rank < as.numeric(x) | rank == as.numeric(x))
my_stopwords <- min_score %>% ungroup %>% distinct(lemma)
return(my_stopwords) 
}


# check auto-generated stop-words list
# when x (rank) = 1, return 13 stop-words
fund_stopwords<- get_stopwords(fund_tf_idf, 1)

# when x (rank) = 2, return 27 stop-words
get_stopwords(fund_tf_idf, 2)

```


```{r}
## huifei's method - percentage as a threshold


# get_percent_stopwords <- function(my_table,x){
#   # mytable is the tf-idf summary table includes columns "word", "tf", "idf" and "tf_idf"
#   # x is the digit and x% of the total word list with least tf-idf score would be listed as stop words
#   limit <- ceiling(0.01*as.numeric(x)*nrow(my_table))
#   mylist <- unique(my_table[1:limit,]$word)
#   return(mylist)
# }

# try:
get_percent_stopwords <- function(my_table,x){
  # mytable is the tf-idf summary table includes columns "word", "tf", "idf" and "tf_idf"
  # x is the digit and x% of the total word list with least tf-idf score would be listed as stop words
  limit <- ceiling(0.01*as.numeric(x)*nrow(my_table))
  mylist <- unique(my_table[1:limit,]$lemma)
  return(mylist)
}

# 8 stop-words
get_percent_stopwords(fund_tf_idf,3)
# length(get_percent_stopwords(fund_tf_idf,3))

# 33 stop-words
get_percent_stopwords(fund_tf_idf,5)
# length(get_percent_stopwords(fund_tf_idf,5))


# 228 stop-words
get_percent_stopwords(fund_tf_idf,10)
# length(get_percent_stopwords(fund_tf_idf,10))
```



# LDA model fitting


# Evaluate model: Coherence matrix
look at the average coherence score

for n topics, use PCA/ t-SNE for dimension reduction, plot 2 dimensions for clustering
