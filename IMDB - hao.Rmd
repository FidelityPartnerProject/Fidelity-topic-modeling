---
title: "IMDB EDA"
author: "Hao He"
date: "2022-10-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(tidytext)
library(ggplot2)
library(topicmodels)
library(quanteda)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(readr)
library(hunspell)
library(textstem)
```

# EDA

First, import data and break down into words. Tokenizing at word level. We’re treating every review as a separate “document”
```{r}

imdb <- read.csv("IMDB Dataset.csv", stringsAsFactors = FALSE)
imdb<- imdb %>% mutate(review_id = row_number()) %>% select(-sentiment)

# try sampling: 1000 reviews
# set.seed(456)
# imdb <- imdb %>% slice_sample(n = 1000, replace = FALSE)


# clean data 
clean<- function (x , pattern){gsub(pattern, " ", x)}

imdb$review <- clean(imdb$review, "<br />")

# unnest into words, keep the CAPS for sentiment analysis 
# tidy_imdb_words <- imdb %>% unnest_tokens(word, review, to_lower = FALSE) %>% anti_join(stop_words)

# #most common words
# tidy_imdb_words %>% 
# count(word, sort = TRUE)
# 
# #plot frequencies
# tidy_imdb_words %>%
#   count(word, sort = TRUE) %>%
#   # filter n = 9000 for the whole dataset
#   filter(n > 9000) %>%
#   mutate(word = reorder(word, n)) %>%
#   ggplot(aes(n, word)) +
#   geom_col() +
#   labs(y = NULL)
```
If we keep CAPS we can see the most common words include some words that is not very meaningful, so we continue with converting to lower cases.



```{r}
tidy_imdb_words<- imdb %>% unnest_tokens(word, review) %>% anti_join(stop_words)

# # customize stop words -  Manually
# stop_words <- bind_rows(tibble(word = c("br", "film", "movie", "films", "movies", "characters", "character", "story", "time","people", "watching", "scene", "scenes", "plot","watch", "real", "cast", "director", "lot", "pretty", "10", "actors", "1", "oz", "makes", "2"), lexicon = c("custom")), stop_words)

tidy_imdb_words%>% count(word, sort = TRUE) %>% 
   # filter n = 9000 for the whole dataset
  filter(n > 9000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```


# Stemming & lemmatization
```{r}
library(textstem)
lemmatization <- tidy_imdb_words %>% mutate(lemma = word%>% lemmatize_words())
library(SnowballC)
stemming <- tidy_imdb_words %>% mutate(stem = word%>% wordStem())
```

# Tf-idf
```{r}
lemma_review_words <- lemmatization %>% count(review_id, lemma, sort = TRUE) 

stem_review_words <- stemming %>% count(review_id, stem, sort = TRUE) 

# calculate the tf separately
# `total` column means total counts of most common words in each document   
# total_words <- lemmatization %>% count(review_id, lemma, sort = TRUE) %>% 
#   group_by(review_id) %>% summarize(total = sum(n))
# 
# review_words <- left_join(review_words,total_words)
# review_words

# lemma tf-idf
lemma_tf_idf <- lemma_review_words %>% 
  bind_tf_idf(lemma, review_id, n) %>% 
  arrange(tf_idf)
lemma_tf_idf  

# stem tf-idf
stem_tf_idf <- stem_review_words %>% 
  bind_tf_idf(stem, review_id, n) %>% 
  arrange(tf_idf)
stem_tf_idf  

# next step: try use percentage as a threshold to automate the identification of stop-words

```

# automation of stop-words

*Apply different method to get stop-words list*

## Rule 1. Based on tf-idf
```{r}
# use the td-idf table after lemmatization
#  rank the tf-idf and remove common words with very low tf-idf score 
min_score <- lemma_tf_idf %>% group_by(review_id) %>% mutate(rank = min_rank(tf_idf)) %>% filter(rank < 1 | rank == 1)
my_stopwords1 <- min_score %>% ungroup %>% distinct(lemma)

# check generated stop-words list
my_stopwords1 

# 33 stop-words for 1000 reviews when rank = 1
# 108 stop-words for 50k reviews 


# use the td-idf table after stemming
# same approach:

stem_min_score <- stem_tf_idf %>% group_by(review_id) %>% mutate(rank = min_rank(tf_idf)) %>% filter(rank < 1 | rank == 1)
stem_my_stopwords1 <- stem_min_score %>% ungroup %>% distinct(stem)

stem_my_stopwords1 
# check generated stop-words list
stem_my_stopwords1 
#104 stop-words for 50k reviews
```

## Rule 2. Based on idf
```{r}
# use lemma_tf_idf table
min_score <- lemma_tf_idf %>% group_by(review_id) %>% mutate(rank = min_rank(idf)) %>% filter(rank < 1 | rank == 1)
my_stopwords2 <- min_score %>% ungroup %>% distinct(lemma)

# check generated stop-words list
my_stopwords2 

# 92 stop-words for 1000 reviews when rank = 1
# 26 stop-words for 50k reviews


# use stem_tf_idf table

stem_min_score <- stem_tf_idf %>% group_by(review_id) %>% mutate(rank = min_rank(idf)) %>% filter(rank < 1 | rank == 1)
stem_my_stopwords1 <- stem_min_score %>% ungroup %>% distinct(stem)

# check generated stop-words list
stem_my_stopwords1 
# 91 stop-words for 50k reviews

```


Now write a function for faster execution using tf-idf rule:
1. first try lemma_tf_idf table
```{r}
# create a general function for automation of stop-words
find_stopwords <- function(tf_idf_table, x){ 
  # tf_idf_table is the tf-idf table has column names in the order of "id", "lemma","n", "tf", "idf" and "tf_idf" 
  # id is the first column name of the tf-idf table, it refers to the column that lists the unique id of each document
  # x is the ranking of elements order by tf-idf in an ascending order.
  # So x = 1 would draw out elements with the lowest tf-idf score; x = 2 would get elements with the second lowest tf-idf score.
  # min_score is a data frame that contains the elements met the ranking demand  
id = colnames(tf_idf_table)[1] 
id <- enquo(id)
min_score <- tf_idf_table %>% group_by_at(vars(!!id)) %>% mutate(rank = min_rank(tf_idf)) %>% filter(rank < as.numeric(x) | rank == as.numeric(x))
my_stopwords <- min_score %>% ungroup %>% distinct(lemma)
return(my_stopwords) 
}


# for 50k reviews, 108 stop-words when x (rank) = 1
find_stopwords(lemma_tf_idf, 1)
# for 1000 reviews, 33 stop-words when x = 1



# for 50k reviews, 218 stop-words when x =2
find_stopwords(lemma_tf_idf, 2)

# for 1000 reviews, 60 stop-words when x = 2

```
2. then try stem_tf_idf table
```{r}
find_stopwords <- function(tf_idf_table, x){ 
  # tf_idf_table is the tf-idf table has column names in the order of "id", "stem","n", "tf", "idf" and "tf_idf" 
  # id is the first column name of the tf-idf table, it refers to the column that lists the unique id of each document
  # x is the ranking of elements order by tf-idf in an ascending order.
  # So x = 1 would draw out elements with the lowest tf-idf score; x = 2 would get elements with the second lowest tf-idf score.
  # min_score is a data frame that contains the elements met the ranking demand  
id = colnames(tf_idf_table)[1] 
id <- enquo(id)
min_score <- tf_idf_table %>% group_by_at(vars(!!id)) %>% mutate(rank = min_rank(tf_idf)) %>% filter(rank < as.numeric(x) | rank == as.numeric(x))
my_stopwords <- min_score %>% ungroup %>% distinct(stem)
return(my_stopwords) 
}

# for 50k reviews, 104 stop-words when x (rank) = 1
find_stopwords(stem_tf_idf, 1)


# for 50k reviews, 217 stop-words when x =2
find_stopwords(stem_tf_idf, 2)


```


Now write a function for faster execution using idf rule
1. first try use lemma_tf_idf table:
```{r}
# create a general function for automation of stop-words
idf_get_stopwords <- function(tf_idf_table, x){ 
  # tf_idf_table is the tf-idf table has column names in the order of "id", "lemma","n","total", "tf", "idf" and "tf_idf" 
  # id is the first column name of the tf-idf table, it refers to the column that lists the unique id of each document
  # x is the ranking of elements order by tf-idf in an ascending order.
  # So x = 1 would draw out elements with the lowest tf-idf score; x = 2 would get elements with the second lowest tf-idf score.
  # min_score is a data frame that contains the elements met the ranking demand  
id = colnames(tf_idf_table)[1] 
id <- enquo(id)
min_score <- tf_idf_table %>% group_by_at(vars(!!id)) %>% mutate(rank = min_rank(idf)) %>% filter(rank < as.numeric(x) | rank == as.numeric(x))
my_stopwords <- min_score %>% ungroup %>% distinct(lemma)
return(my_stopwords) 
}


# check auto-generated stop-words list
# for 50k reviews, return 92 stop-words when x (rank) = 1
idf_get_stopwords(lemma_tf_idf, 1) 


# for 50k reviews, return 197 stop-words, when x (rank) = 2
idf_get_stopwords(lemma_tf_idf, 2)



```
2. then use stem_tf_idf table:
```{r}
# create a general function for automation of stop-words
idf_get_stopwords <- function(tf_idf_table, x){ 
  # tf_idf_table is the tf-idf table has column names in the order of "id", "lemma","n","total", "tf", "idf" and "tf_idf" 
  # id is the first column name of the tf-idf table, it refers to the column that lists the unique id of each document
  # x is the ranking of elements order by tf-idf in an ascending order.
  # So x = 1 would draw out elements with the lowest tf-idf score; x = 2 would get elements with the second lowest tf-idf score.
  # min_score is a data frame that contains the elements met the ranking demand  
id = colnames(tf_idf_table)[1] 
id <- enquo(id)
min_score <- tf_idf_table %>% group_by_at(vars(!!id)) %>% mutate(rank = min_rank(idf)) %>% filter(rank < as.numeric(x) | rank == as.numeric(x))
my_stopwords <- min_score %>% ungroup %>% distinct(stem)
return(my_stopwords) 
}


# check auto-generated stop-words list
# for 50k reviews, return 91 stop-words when x (rank) = 1
idf_get_stopwords(stem_tf_idf, 1) 


# for 50k reviews, return 191 stop-words, when x (rank) = 2
idf_get_stopwords(stem_tf_idf, 2)


```


### Other methods
Method 2
```{r}
## huifei's method - percentage as a threshold


# get_percent_stopwords <- function(my_table,x){
#   # mytable is the tf-idf summary table includes columns "word", "tf", "idf" and "tf_idf"
#   # x is the digit and x% of the total word list with least tf-idf score would be listed as stop words
#   limit <- ceiling(0.01*as.numeric(x)*nrow(my_table))
#   mylist <- unique(my_table[1:limit,]$word)
#   return(mylist)
# }


get_percent_stopwords <- function(my_table,x){
  # mytable is the tf-idf summary table includes columns "word", "tf", "idf" and "tf_idf"
  # x is the digit and x% of the total word list with least tf-idf score would be listed as stop words
  limit <- ceiling(0.01*as.numeric(x)*nrow(my_table))
  mylist <- unique(my_table[1:limit,]$word)
  return(mylist)
}

lemma_get_percent_stopwords<- function(my_table,x){
  # mytable is the tf-idf summary table includes columns "word", "tf", "idf" and "tf_idf"
  # x is the digit and x% of the total word list with least tf-idf score would be listed as stop words
  limit <- ceiling(0.01*as.numeric(x)*nrow(my_table))
  mylist <- unique(my_table[1:limit,]$lemma)
  return(mylist)
}

# apply function to tf-idf table
# get tf-idf first
review_tf_idf <- tidy_imdb_words %>% 
  count(review_id, word, sort = TRUE) %>%  
  bind_tf_idf(word,review_id,n) %>% 
  arrange(tf_idf)

# 3523 stop-words
get_percent_stopwords(review_tf_idf,3)
# length(get_percent_stopwords(review_tf_idf,3))
# for 1000 reviews, 588 stop-words 


# 5060 stop-words
length(get_percent_stopwords(review_tf_idf,5))
# for 1000 reviews, 1012 stop-words 

# 9647 stop-words
length(get_percent_stopwords(review_tf_idf,10))
# for 1000 reviews, 2190 stop-words


# apply function to tf-idf table after lemmtization
# 2395 stop-words
lemma_get_percent_stopwords(imdb_tf_idf,3)
# length(lemma_get_percent_stopwords(imdb_tf_idf,3))
# for 1000 reviews, 440 stop-words 


# 33 stop-words
lemma_get_percent_stopwords(imdb_tf_idf,5)
# length(lemma_get_percent_stopwords(imdb_tf_idf,5))
# for 1000 reviews, 751 stop-words


# 228 stop-words
lemma_get_percent_stopwords(imdb_tf_idf,10)
# length(lemma_get_percent_stopwords(imdb_tf_idf,10))
# for 1000 reviews, 1598 stop-words

```


Method3
```{r}
# Mia's method
filterOut<-function(dfDF,PropInEachDOC=0.4)
{
   notkey<-dfDF %>% 
            group_by(doc) %>% 
            slice_min(idf,prop=PropInEachDOC)
   
   notkeyword<-notkey %>% 
               group_by(word) %>%
               summarise(n=n()) %>%
               arrange(desc(n)) %>%
         slice_max(n,prop=1/4)
   
   notkeyword
}

```



## n-gram:  
bigram as token
```{r}
bigrams <- imdb %>% unnest_tokens(bigram, review, token = "ngrams", n = 2)
# most common bigrams
bigrams %>%
  count(bigram, sort = TRUE)

bigrams %>%
  count(bigram, sort = TRUE) %>% 
filter(n > 9000) %>%
  mutate(word = reorder(bigram, n)) %>%
  ggplot(aes(n, bigram)) +
  geom_col() +
  labs(y = NULL)

# Separate bigrams
bigrams_separated <- bigrams %>% separate(bigram, c("word1", "word2"), sep = " ")

# remove words that are common by selecting words that is not a stop-word.
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts

# recombine the columns into one: now we have the most common bigrams not containing stop-words.
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```


