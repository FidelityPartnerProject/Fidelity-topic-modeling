---
title: "IMDB EDA"
author: "Hao He"
date: "2022-10-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(tidytext)
library(ggplot2)
library(topicmodels)
library(quanteda)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(readr)
library(hunspell)
library(textstem)
```

# EDA

First, import data and break down into words. Tokenizing at word level. We’re treating every review as a separate “document”
```{r}

imdb <- read.csv("IMDB Dataset.csv", stringsAsFactors = FALSE)
imdb<- imdb %>% mutate(review_id = row_number())
# clean data 
clean<- function (x , pattern){gsub(pattern, " ", x)}

imdb$review <- clean(imdb$review, "<br />")

# unnest into words, keep the CAPS for sentiment analysis 
tidy_imdb_words <- imdb %>% unnest_tokens(word, review, to_lower = FALSE) %>% anti_join(stop_words)

#most common words
tidy_imdb_words %>% 
count(word, sort = TRUE)

#plot frequencies
tidy_imdb_words %>%
  count(word, sort = TRUE) %>%
  filter(n > 9000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```
If we keep CAPS we can see the most common words include some words that is not very meaningful, so we continue with converting to lower cases.

```{r}
tidy_imdb_words<- imdb %>% unnest_tokens(word, review) %>% anti_join(stop_words)

# # customize stop words -  Manually
# stop_words <- bind_rows(tibble(word = c("br", "film", "movie", "films", "movies", "characters", "character", "story", "time","people", "watching", "scene", "scenes", "plot","watch", "real", "cast", "director", "lot", "pretty", "10", "actors", "1", "oz", "makes", "2"), lexicon = c("custom")), stop_words)

tidy_imdb_words%>% count(word, sort = TRUE) %>% filter(n > 9000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```


# Stemming & lemmatization
```{r}
library(textstem)
lemmatization <- tidy_imdb_words %>% mutate(lemma = word%>% lemmatize_words())
stemming <- tidy_imdb_words %>% mutate(stem = word%>% wordStem())
```

# Tf-idf
```{r}
review_words<- lemmatization %>% count(review_id, lemma, sort = TRUE) 

# `total` column means total counts of most common words in each document   
total_words <- lemmatization %>% count(review_id, lemma, sort = TRUE) %>% 
  group_by(review_id) %>% summarize(total = sum(n))

review_words <- left_join(review_words,total_words)
review_words

imdb_tf_idf <- review_words %>% 
  bind_tf_idf(lemma, review_id, n) %>% 
  arrange(tf_idf)
imdb_tf_idf  
# next step: try use percentage as a threshold to automate the identification of stop-words

```

# automation of stop-words

*Apply different method to get stop-words list*

## Rule 1. Based on tf-idf
```{r}
# run the td-idf after lemmatization

#  rank the tf-idf and remove common words with very low tf-idf score 
min_score <- imdb_tf_idf %>% group_by(review_id) %>% mutate(rank = min_rank(tf_idf)) %>% filter(rank < 1 | rank == 1)
my_stopwords1 <- min_score %>% ungroup %>% distinct(lemma)

# check generated stop-words list
my_stopwords1 

# 108 stop-words when rank = 1
```


## Rule 2. Based on idf
```{r}
min_score <- imdb_tf_idf %>% group_by(review_id) %>% mutate(rank = min_rank(idf)) %>% filter(rank < 1 | rank == 1)
my_stopwords2 <- min_score %>% ungroup %>% distinct(lemma)

# check generated stop-words list
my_stopwords2 

# 92 stop-words when rank = 1



```


Now write a function for faster execution using tf-idf rule
```{r}
# create a general function for automation of stop-words
get_stopwords <- function(tf_idf_table, x){ 
  # tf_idf_table is the tf-idf table has column names in the order of "id", "lemma","n","total", "tf", "idf" and "tf_idf" 
  # id is the first column name of the tf-idf table, it refers to the column that lists the unique id of each document
  # x is the ranking of elements order by tf-idf in an ascending order.
  # So x = 1 would draw out elements with the lowest tf-idf score; x = 2 would get elements with the second lowest tf-idf score.
  # min_score is a data frame that contains the elements met the ranking demand  
id = colnames(tf_idf_table)[1] 
id <- enquo(id)
min_score <- tf_idf_table %>% group_by_at(vars(!!id)) %>% mutate(rank = min_rank(tf_idf)) %>% filter(rank < as.numeric(x) | rank == as.numeric(x))
my_stopwords <- min_score %>% ungroup %>% distinct(lemma)
return(my_stopwords) 
}


# check auto-generated stop-words list
# when x (rank) = 1, return 108 stop-words
get_stopwords(imdb_tf_idf, 1)

# when x (rank) = 2, return 218 stop-words
get_stopwords(imdb_tf_idf, 2)

```

Method 2
```{r}
## huifei's method - percentage as a threshold


# get_percent_stopwords <- function(my_table,x){
#   # mytable is the tf-idf summary table includes columns "word", "tf", "idf" and "tf_idf"
#   # x is the digit and x% of the total word list with least tf-idf score would be listed as stop words
#   limit <- ceiling(0.01*as.numeric(x)*nrow(my_table))
#   mylist <- unique(my_table[1:limit,]$word)
#   return(mylist)
# }


get_percent_stopwords <- function(my_table,x){
  # mytable is the tf-idf summary table includes columns "word", "tf", "idf" and "tf_idf"
  # x is the digit and x% of the total word list with least tf-idf score would be listed as stop words
  limit <- ceiling(0.01*as.numeric(x)*nrow(my_table))
  mylist <- unique(my_table[1:limit,]$word)
  return(mylist)
}


# apply function to tf-idf table
review_tf_idf <- tidy_imdb_words %>% 
  count(review_id, word, sort = TRUE) %>%  
  bind_tf_idf(word,review_id,n) %>% 
  arrange(tf_idf)

# 3523 stop-words
length(get_percent_stopwords(review_tf_idf,3))

# 5060 stop-words
length(get_percent_stopwords(review_tf_idf,5))

# 9647 stop-words
length(get_percent_stopwords(review_tf_idf,10))


# ap-ply function to tf-idf table after lemmtization
# 2395 stop-words
length(get_percent_stopwords(imdb_tf_idf,3))
# length(get_percent_stopwords(fund_tf_idf,3))

# 33 stop-words
get_percent_stopwords(fund_tf_idf,5)
# length(get_percent_stopwords(fund_tf_idf,5))


# 228 stop-words
get_percent_stopwords(fund_tf_idf,10)
# length(get_percent_stopwords(fund_tf_idf,10))


```


Method3
```{r}
# Mia's method
filterOut<-function(dfDF,PropInEachDOC=0.4)
{
   notkey<-dfDF %>% 
            group_by(doc) %>% 
            slice_min(idf,prop=PropInEachDOC)
   
   notkeyword<-notkey %>% 
               group_by(word) %>%
               summarise(n=n()) %>%
               arrange(desc(n)) %>%
         slice_max(n,prop=1/4)
   
   notkeyword
}

```



## n-gram:  
bigram as token
```{r}
bigrams <- imdb %>% unnest_tokens(bigram, review, token = "ngrams", n = 2)
# most common bigrams
bigrams %>%
  count(bigram, sort = TRUE)

bigrams %>%
  count(bigram, sort = TRUE) %>% 
filter(n > 9000) %>%
  mutate(word = reorder(bigram, n)) %>%
  ggplot(aes(n, bigram)) +
  geom_col() +
  labs(y = NULL)

# Separate bigrams
bigrams_separated <- bigrams %>% separate(bigram, c("word1", "word2"), sep = " ")

# remove words that are common by selecting words that is not a stop-word.
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts

# recombine the columns into one: now we have the most common bigrams not containing stop-words.
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```


